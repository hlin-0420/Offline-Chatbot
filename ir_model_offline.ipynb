{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: haystack-ai in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (2.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: haystack-experimental in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (0.8.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (4.23.0)\n",
      "Requirement already satisfied: lazy-imports in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (0.4.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (10.6.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (2.2.4)\n",
      "Requirement already satisfied: openai>=1.56.1 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (1.68.0)\n",
      "Requirement already satisfied: posthog!=3.12.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (3.21.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (2.10.6)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (9.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from haystack-ai) (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (1.3.1)\n",
      "Requirement already satisfied: sounddevice>=0.5.1 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from openai>=1.56.1->haystack-ai) (0.5.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (1.17.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from posthog!=3.12.0->haystack-ai) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from pydantic->haystack-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from pydantic->haystack-ai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from requests->haystack-ai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from requests->haystack-ai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from requests->haystack-ai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from requests->haystack-ai) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from tqdm->haystack-ai) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from jinja2->haystack-ai) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from jsonschema->haystack-ai) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from jsonschema->haystack-ai) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from jsonschema->haystack-ai) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from jsonschema->haystack-ai) (0.23.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.56.1->haystack-ai) (0.14.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from sounddevice>=0.5.1->openai>=1.56.1->haystack-ai) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.5.1->openai>=1.56.1->haystack-ai) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 haystack-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = \"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_htm_files():\n",
    "    \"\"\"\n",
    "    Recursively finds all .htm files in the base directory and its subdirectories.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of file paths relative to the base directory.\n",
    "    \"\"\"\n",
    "    htm_files = []\n",
    "    for root, _, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".htm\"):\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), start=base_directory)\n",
    "                htm_files.append(os.path.join(base_directory, relative_path))\n",
    "    \n",
    "    return htm_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "htm_files = _list_htm_files() # DEMO: Listing all the htm_files in the Data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data\\\\Contact_us.htm',\n",
       " 'Data\\\\csh-redirect.htm',\n",
       " 'Data\\\\First_Topic.htm',\n",
       " 'Data\\\\Help_Missing.htm',\n",
       " 'Data\\\\index.htm',\n",
       " 'Data\\\\topic.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Drilling\\\\Cement_Volume.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Drilling\\\\D_Exponent.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Drilling\\\\Mechanical_Specific_Energy.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Drilling\\\\Temperature_Gradient.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\C1_Sum.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Gas_Balance.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Gas_Character.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Gas_Wetness.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Inverse_Oil_Indicator.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Oil_Indicator.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Pixler_C1_C2_Gas_Ratio.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Pixler_C1_C3_Gas_Ratio.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Pixler_C1_C4_Gas_Ratio.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Formation_Fluid_Evaluation\\\\Pixler_C1_C5_Gas_Ratio.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\C1_Moving_Average.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\C2_Moving_Average.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\Delta_C1_Moving_Average.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\Delta_C2_Moving_Average.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\Delta_Total_Gas_Moving_Average.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\Gas_Quality_Ratio.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\Total_Gas_in_PPM.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\Total_Gas_Moving_Average.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Gas\\\\Gas_Quality\\\\Total_Gas_Normalization.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Net_Pay_(Sw_using_ARCHIE,_Rw_from_SP).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Net_Pay_(Sw_using_ARCHIE,_Rw_known).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Net_Pay_(Sw_using_INDONESIAN,_Rw_from_SP).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Net_Pay_(Sw_using_INDONESIAN,_Rw_known).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Net_Pay_(Sw_using_SCHLUMBERGER,_Rw_from_SP).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Net_Pay_(Sw_using_SCHLUMBERGER,_Rw_known).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Net_Pay_(Sw_using_SIMANDOUX,_Rw_from_SP).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Net_Pay_(Sw_using_SIMANDOUX,_Rw_known).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Porosity_Effective_(Clean_Zone).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Porosity_Effective_(with_Shale_Correction).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Porosity_from_Density_(Clean_Zone).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Porosity_from_Density_(with_Shale_Correction).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Porosity_from_Sonic_(Clean_Zone).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Porosity_from_Sonic_(with_Shale_Correction).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Rw_at_Formation_Temperature.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\RW_from_SP.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Shale_Volume_from_GR.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Sw_using_ARCHIE_(Rw_from_SP).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Sw_using_ARCHIE_(Rw_known).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Sw_using_INDONESIAN_(Rw_from_SP).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Sw_using_INDONESIAN_(Rw_known).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Sw_using_SCHLUMBERGER_(Rw_from_SP).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Sw_using_SCHLUMBERGER_(Rw_known).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Sw_using_SIMANDOUX_(Rw_from_SP).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Sw_using_SIMANDOUX_(Rw_known).htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\Petrophysical\\\\Th_K_Ratio.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\TVD_&_Well_Trajectory\\\\Easting.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\TVD_&_Well_Trajectory\\\\Northing.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\TVD_&_Well_Trajectory\\\\TVD,_using_Minimum_Curvature_method.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\TVD_&_Well_Trajectory\\\\TVD_Average_Tangential.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\TVD_&_Well_Trajectory\\\\Vertical_Sect.htm',\n",
       " 'Data\\\\Computed_Curve_Templates\\\\TVD_&_Well_Trajectory\\\\Well_Departure.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Create_Curve_Data_from_Table_Columns.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Create_Cutoff_Curve.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Create_Multiple_Curve_Data.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Create_Polyline_Curve.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Create_User-defined_Curve_or_Gradi.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Curve_Comp.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Edit_Curve_Data.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Generate_Integrated_Travel_Time_Pips.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\Mouse_Set_Curve_Data.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\View_and_Edit_Existing_Multiple_Curve_Groups.htm',\n",
       " 'Data\\\\Create_Curve_Data\\\\View_Curve_Data_History.htm',\n",
       " 'Data\\\\Curve_Data\\\\Automatic_File_Load.htm',\n",
       " 'Data\\\\Curve_Data\\\\Comma_and_Tab-delimited_ASCII_Files.htm',\n",
       " 'Data\\\\Curve_Data\\\\Convert_LIS_to_LAS.htm',\n",
       " 'Data\\\\Curve_Data\\\\Data_Import_Wizard_Step_1.htm',\n",
       " 'Data\\\\Curve_Data\\\\Loading_XML_Files_(WellLogML).htm',\n",
       " 'Data\\\\Curve_Data\\\\Load_Las_Curve_Data.htm',\n",
       " 'Data\\\\Curve_Data\\\\Load_Space-delimited_ASCII_Files.htm',\n",
       " 'Data\\\\Curve_Display\\\\Alter_the_Order_of_Curve_Legends.htm',\n",
       " 'Data\\\\Curve_Display\\\\Apply_Curve_Display_Settings_(Defaults).htm',\n",
       " 'Data\\\\Curve_Display\\\\Automatic_Curve_Populate.htm',\n",
       " 'Data\\\\Curve_Display\\\\Change_Curve_Attributed_using_Context_Tab.htm',\n",
       " 'Data\\\\Curve_Display\\\\Change_Curve_Pens.htm',\n",
       " 'Data\\\\Curve_Display\\\\Create_Rose_Plots.htm',\n",
       " 'Data\\\\Curve_Display\\\\Curve_as_Text.htm',\n",
       " 'Data\\\\Curve_Display\\\\Curve_Types_in_GEO.htm',\n",
       " 'Data\\\\Curve_Display\\\\Define_New_Curve_Pens.htm',\n",
       " 'Data\\\\Curve_Display\\\\Display_Data_Point_Markers.htm',\n",
       " 'Data\\\\Curve_Display\\\\Display_Wrapped_Curve_Data.htm',\n",
       " 'Data\\\\Curve_Display\\\\Label_the_Curve.htm',\n",
       " 'Data\\\\Curve_Display\\\\Modify_a_Tadpole_Curve.htm',\n",
       " 'Data\\\\Curve_Display\\\\Multiple_Scales_for_Curve.htm',\n",
       " 'Data\\\\Curve_Display\\\\Name_the_Curve.htm',\n",
       " 'Data\\\\Curve_Display\\\\Present_Curves_on_the_Log.htm',\n",
       " 'Data\\\\Curve_Display\\\\Present_Curve_in_Tadpole_Format.htm',\n",
       " 'Data\\\\Curve_Display\\\\Save_the_Curve_Display_Settings_(Defaults).htm',\n",
       " 'Data\\\\Curve_Shading,_Splicing_and_Depth_Shifting\\\\Apply_Curve_Shading.htm',\n",
       " 'Data\\\\Curve_Shading,_Splicing_and_Depth_Shifting\\\\Apply_Lithologies_as_Curve_Shading.htm',\n",
       " 'Data\\\\Curve_Shading,_Splicing_and_Depth_Shifting\\\\Curve_Depth_Shifting.htm',\n",
       " 'Data\\\\Curve_Shading,_Splicing_and_Depth_Shifting\\\\Shade_Wrapped_Curve.htm',\n",
       " 'Data\\\\Curve_Shading,_Splicing_and_Depth_Shifting\\\\Splice.htm',\n",
       " 'Data\\\\Curve_Shading_and_Splicing\\\\Apply_Curve_Shading.htm',\n",
       " 'Data\\\\Curve_Shading_and_Splicing\\\\Apply_Lithologies_as_Curve_Shading.htm',\n",
       " 'Data\\\\Curve_Shading_and_Splicing\\\\Shade_Wrapped_Curve.htm',\n",
       " 'Data\\\\Curve_Shading_and_Splicing\\\\Splice.htm',\n",
       " 'Data\\\\Free_Format_Text\\\\Add_Free_Format_Text.htm',\n",
       " 'Data\\\\Free_Format_Text\\\\Delete_Free_Format_Text.htm',\n",
       " 'Data\\\\Free_Format_Text\\\\Edit_Free_Format.htm',\n",
       " 'Data\\\\Free_Format_Text\\\\Moving_Free_Format_Text.htm',\n",
       " 'Data\\\\GEOGraph\\\\Add_a_Graph.htm',\n",
       " 'Data\\\\GEOGraph\\\\GEOGraph_Introduction.htm',\n",
       " 'Data\\\\Headers_and_Trailers\\\\Add_Headers_and_Trailers.htm',\n",
       " 'Data\\\\Headers_and_Trailers\\\\Add_Header_Data.htm',\n",
       " 'Data\\\\Headers_and_Trailers\\\\Fonts_for_Vector_Objects.htm',\n",
       " 'Data\\\\Headers_and_Trailers\\\\Header_and_Trailer_Display_Hierarchy.htm',\n",
       " 'Data\\\\Headers_and_Trailers\\\\Load_a_Header_or_Trailer.htm',\n",
       " 'Data\\\\Headers_and_Trailers\\\\Selecting_Fonts.htm',\n",
       " 'Data\\\\Headers_and_Trailers\\\\Mnemonics_for_Header_and_Trailers\\\\Mnemonics_in_G.htm',\n",
       " 'Data\\\\Headers_and_Trailers\\\\Mnemonics_for_Header_and_Trailers\\\\System_Variables_in_Header_Mnemonics.htm',\n",
       " 'Data\\\\Horizontal_Track_Text_-_Lithology_Descriptions_or_Remarks\\\\Add_Lithology_Descriptions_or_Remarks.htm',\n",
       " 'Data\\\\Horizontal_Track_Text_-_Lithology_Descriptions_or_Remarks\\\\Delete_Lithology_Descriptions_and_Remarks.htm',\n",
       " 'Data\\\\Horizontal_Track_Text_-_Lithology_Descriptions_or_Remarks\\\\Edit_Lithology_Descriptions_or_Remarks_Text.htm',\n",
       " 'Data\\\\Horizontal_Track_Text_-_Lithology_Descriptions_or_Remarks\\\\Import_Lithology_Descriptions_and_Remarks_Text.htm',\n",
       " 'Data\\\\Horizontal_Track_Text_-_Lithology_Descriptions_or_Remarks\\\\Move_Lithology_Descriptions_or_Remarks_Text.htm',\n",
       " 'Data\\\\Introduction\\\\Application_Look.htm',\n",
       " 'Data\\\\Introduction\\\\GEO_License_and_Version_Number.htm',\n",
       " 'Data\\\\Introduction\\\\GEO_Limits.htm',\n",
       " 'Data\\\\Introduction\\\\RUS_Rental_Dongle_Guide.htm',\n",
       " 'Data\\\\Introduction\\\\System_Requirements.htm',\n",
       " 'Data\\\\Lines\\\\Add_L.htm',\n",
       " 'Data\\\\Lines\\\\Delete_Line.htm',\n",
       " 'Data\\\\Lines\\\\Edit_Line.htm',\n",
       " 'Data\\\\Lines\\\\Move_Line.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Append_Curve_Data.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Automatic_File_Load.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Comma_and_Tab-delimited_ASCII_Files.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Convert_LIS_to_LAS.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Data_Import_Wizard_Step_1.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\General_Rules_of_Loadig_Data.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Loading_XML_Files_(WellLogML).htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Load_Las_Curve_Data.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Load_Lis_or_Binary_Files.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Load_Space-delimited_ASCII_Files.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Load_Text_Curve_Data.htm',\n",
       " 'Data\\\\Load_Curve_Data\\\\Types_of_Curve_Data.htm',\n",
       " 'Data\\\\Log_Presentation_and_Layout\\\\Create_a_Qualitative_Track.htm',\n",
       " 'Data\\\\Log_Presentation_and_Layout\\\\Create_Multiple_Layouts.htm',\n",
       " 'Data\\\\Log_Presentation_and_Layout\\\\Create_Presentation_or_Track_Format.htm',\n",
       " 'Data\\\\Log_Presentation_and_Layout\\\\Import_Qualitative_Data.htm',\n",
       " 'Data\\\\Log_Presentation_and_Layout\\\\Layout_Items.htm',\n",
       " 'Data\\\\Log_Presentation_and_Layout\\\\Manage_Layouts.htm',\n",
       " 'Data\\\\Log_Presentation_and_Layout\\\\Shifting_Tracks.htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Change_Plot_Index.htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Date-Time_Formats.htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Depth_tab_(for_Depth_Plots).htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Depth_Tab_(for_Time_Plots).htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Global_Settings_for_Date-Time_Plots.htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Global_Settings_for_Depth_Plots.htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Interpreted_Lithology\\\\Add_Lithology.htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Interpreted_Lithology\\\\Delete_Lithology.htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Interpreted_Lithology\\\\Edit_Lithology.htm',\n",
       " 'Data\\\\Log_Structure_and_Presentation\\\\Templates_(ODT)\\\\Create_Template_using_Wizard.htm',\n",
       " 'Data\\\\Modifiers_and_Qualifiers\\\\Add_Modifiers.htm',\n",
       " 'Data\\\\Modifiers_and_Qualifiers\\\\Add_Qualifiers.htm',\n",
       " 'Data\\\\Modifiers_and_Qualifiers\\\\Delete_modifier.htm',\n",
       " 'Data\\\\Modifiers_and_Qualifiers\\\\Delete_Qualifiers.htm',\n",
       " 'Data\\\\Modifiers_and_Qualifiers\\\\Move_Modifiers.htm',\n",
       " 'Data\\\\Navigate_GEO\\\\GEO_Menu.htm',\n",
       " 'Data\\\\Navigate_GEO\\\\Log_On_as_GEO_User.htm',\n",
       " 'Data\\\\Navigate_GEO\\\\Quick_Access_Toolbar.htm',\n",
       " 'Data\\\\Navigate_GEO\\\\Ribbons_Menu.htm',\n",
       " 'Data\\\\Navigate_GEO\\\\Split_Screen_GEO.htm',\n",
       " 'Data\\\\Navigate_GEO\\\\Starting_a_new_GEO_session.htm',\n",
       " 'Data\\\\Navigate_GEO\\\\Status_Bar.htm',\n",
       " 'Data\\\\Navigate_GEO\\\\Title_Bar.htm',\n",
       " 'Data\\\\Percent_Lithology\\\\Add_Percent_Lithology.htm',\n",
       " 'Data\\\\Percent_Lithology\\\\Add_Percent_Lithology_Descriptions.htm',\n",
       " 'Data\\\\Percent_Lithology\\\\Export_Percent_Lithology_to_ASCII.htm',\n",
       " 'Data\\\\Percent_Lithology\\\\Export_Percent_Lithology_to_LAS.htm',\n",
       " 'Data\\\\Percent_Lithology\\\\Import_Percent_Lithology.htm',\n",
       " 'Data\\\\Percent_Lithology\\\\Shape_Percent_Lithology_with_the_Mouse.htm',\n",
       " 'Data\\\\Print\\\\Print.htm',\n",
       " 'Data\\\\Print\\\\Print_to_PDF.htm',\n",
       " 'Data\\\\Qualitative_Data\\\\Add_Qualitative_Data.htm',\n",
       " 'Data\\\\Qualitative_Data\\\\Export_Qualitative_Data.htm',\n",
       " 'Data\\\\Qualitative_Data\\\\Import_Qualitative_Data.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Align_Text.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Apply_Highlighting.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Change_Indents_and_Spacing.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Change_Text_Line_Spacing.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Change_the_Font_Color.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Change_the_Font_Family.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Change_the_Font_Size.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Format_Text_Using_the_Format_Text_Tab.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Format_Text_Using_the_Rich_Edit_Control.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Insert_a_Subscript_Character.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Insert_a_Superscript_Character.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Make_the_Text_Bold.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Make_the_Text_Italic.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Start_a_List.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Strikethrough_the_Text.htm',\n",
       " 'Data\\\\Rich_Format_Text\\\\Underline_Text.htm',\n",
       " 'Data\\\\Sharing\\\\e-View_Enable_an_ODF.htm',\n",
       " 'Data\\\\Sharing\\\\Event_Log.htm',\n",
       " 'Data\\\\Sharing\\\\Jumping_to_a_certain_depth.htm',\n",
       " 'Data\\\\Sharing\\\\Login_for_Access_to_Document.htm',\n",
       " 'Data\\\\Sharing\\\\Protecting_your_ODF_File.htm',\n",
       " 'Data\\\\Sharing\\\\Create_a_Report_Link_to_an_Object_in_the_Log\\\\Links_Mana.htm',\n",
       " 'Data\\\\Sharing\\\\Create_a_Report_Link_to_an_Object_in_the_Log\\\\Object_Links_in_GEO.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Baggage_File_Mana.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_All_Other_Data_(except_curve_data).htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_Baggage_and_Library.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_Curve_Data.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_Curve_Data_to_XML_Format.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_Data_to_WitsML.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_to_Image_File\\\\Export_GEO_Plot_as_Image.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_to_Image_File\\\\Export_GEO_Plot_to_CGM.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_to_Image_File\\\\Export_GEO_Plot_to_EM.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_to_Image_File\\\\Export_GEO_Plot_to_SVG.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_to_Image_File\\\\Export_GEO_Plot_to_TIFF.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_to_Image_File\\\\Export_GEO_Plot_to_WMF.htm',\n",
       " 'Data\\\\Sharing\\\\Export_Data_and_Information\\\\Export_to_Image_File\\\\Export_to_HTM.htm',\n",
       " 'Data\\\\Sharing\\\\ODF_Update_File_(OUF))\\\\Concept_of_ODF_Update_File_(OUF).htm',\n",
       " 'Data\\\\Sharing\\\\ODF_Update_File_(OUF))\\\\Create_or_Save_an_OU.htm',\n",
       " 'Data\\\\Sharing\\\\ODF_Update_File_(OUF))\\\\Load_OUF_Files.htm',\n",
       " 'Data\\\\Sharing\\\\ODF_Update_File_(OUF))\\\\OUF_Auto_merging.htm',\n",
       " 'Data\\\\Sharing\\\\ODF_Update_File_(OUF))\\\\OUF_Resave_Feature.htm',\n",
       " 'Data\\\\Sharing\\\\ODF_Update_File_(OUF))\\\\Send_OUF_as_mail_attachment.htm',\n",
       " 'Data\\\\Sharing\\\\Publish_ODF_to_Web_Folder\\\\VUF_File_Transfe.htm',\n",
       " 'Data\\\\Sidetrack\\\\Create_Sidetrac.htm',\n",
       " 'Data\\\\Symbols\\\\Add_Symbols.htm',\n",
       " 'Data\\\\Symbols\\\\Delete_Symbol.htm',\n",
       " 'Data\\\\Symbols\\\\Move_Symbol.htm',\n",
       " 'Data\\\\Tables\\\\Add_Table_Data.htm',\n",
       " 'Data\\\\Tables\\\\Create_or_Amend_Table_Templates.htm',\n",
       " 'Data\\\\Tables\\\\Export_Table.htm',\n",
       " 'Data\\\\Tables\\\\Geotable.htm',\n",
       " 'Data\\\\Tables\\\\Import_table_data.htm',\n",
       " 'Data\\\\Tables\\\\Navigate_Table_Definitions.htm',\n",
       " 'Data\\\\Tables\\\\Place_Symbols_from_Table_Data.htm',\n",
       " 'Data\\\\Tables\\\\Replacing_an_Old_Table_Template_with_a_New_One.htm',\n",
       " 'Data\\\\Text_and_Annotations\\\\Geological_Dictionary.htm',\n",
       " 'Data\\\\Text_and_Annotations\\\\Spell_Check.htm',\n",
       " 'Data\\\\Touch_Screen_Devices\\\\Touch_Screen_Devices.htm',\n",
       " 'Data\\\\TVD\\\\Index_Convertor.htm',\n",
       " 'Data\\\\TVD\\\\Presenting_TVD.htm',\n",
       " 'Data\\\\Vertical_Track_Text_-_Chronological or Stratigraphic\\\\Add_Chronological_or_Stratigraphic_Text.htm',\n",
       " 'Data\\\\Vertical_Track_Text_-_Chronological or Stratigraphic\\\\Adjust_Depth_Range_of_Chronological_or_Stratigraphic_text.htm',\n",
       " 'Data\\\\Vertical_Track_Text_-_Chronological or Stratigraphic\\\\Delete_Chronological_or_Stratigraphic_Text.htm',\n",
       " 'Data\\\\Vertical_Track_Text_-_Chronological or Stratigraphic\\\\Edit_Chronological_and_Stratigraphic_Text.htm',\n",
       " 'Data\\\\Vertical_Track_Text_-_Chronological or Stratigraphic\\\\Manage_Stratigraphy.htm',\n",
       " 'Data\\\\VOB_Files\\\\Concept_of_Vector_Objects_(VOB).htm',\n",
       " 'Data\\\\VOB_Files\\\\Vector_Object_Language_Compiler_(VLCP).htm',\n",
       " 'Data\\\\Working_with_Files\\\\Close_an_ODF.htm',\n",
       " 'Data\\\\Working_with_Files\\\\Files_in_GEO.htm',\n",
       " 'Data\\\\Working_with_Files\\\\File_Types_Handled_by_GEO.htm',\n",
       " 'Data\\\\Working_with_Files\\\\Load_a_VEW_File.htm',\n",
       " 'Data\\\\Working_with_Files\\\\ODF_Interval_Files_(OIF).htm',\n",
       " 'Data\\\\Working_with_Files\\\\ODF_Template_File_(ODT).htm',\n",
       " 'Data\\\\Working_with_Files\\\\ODF_Update_File_(OUF).htm',\n",
       " 'Data\\\\Working_with_Files\\\\Open_an_ODF_or_VEW_File.htm',\n",
       " 'Data\\\\Working_with_Files\\\\Open_an_ODT_File.htm',\n",
       " 'Data\\\\Working_with_Files\\\\Output_Database_File_(ODF).htm',\n",
       " 'Data\\\\Working_with_Files\\\\Save_an_ODF.htm',\n",
       " 'Data\\\\Working_with_Files\\\\Save_an_ODT_File.htm',\n",
       " 'Data\\\\Working_with_Files\\\\Save_a_VEW_File.htm',\n",
       " 'Data\\\\Working_with_Files\\\\View_File_(VEW).htm',\n",
       " 'Data\\\\Working_with_Files\\\\View_ODF_files_in_EView.htm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htm_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tabulate in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(soup):\n",
    "    # Define navigation-related keyword patterns\n",
    "    navigation_keywords = [\n",
    "        r'contact\\s+us', r'click\\s+(here|for)', r'guidance', r'help', r'support', r'assistance',\n",
    "        r'maximize\\s+screen', r'view\\s+details', r'read\\s+more', r'convert.*file', r'FAQ', r'learn\\s+more'\n",
    "    ]\n",
    "    \n",
    "    navigation_pattern = re.compile(r\"|\".join(navigation_keywords), re.IGNORECASE)\n",
    "\n",
    "    # Remove navigation-related text\n",
    "    for tag in soup.find_all(\"p\"):\n",
    "        if navigation_pattern.search(tag.text):\n",
    "            tag.decompose()\n",
    "\n",
    "    # Extract only meaningful paragraph text (excluding very short ones)\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 20]\n",
    "    \n",
    "    clean_text = \"\\n\\n\".join(paragraphs)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_list(soup):\n",
    "    # Extract lists properly\n",
    "    lists = []\n",
    "    for ul in soup.find_all(\"ul\"):\n",
    "        items = [li.get_text(strip=True) for li in ul.find_all(\"li\")]\n",
    "        lists.append(items)\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_as_text_block(file_path):\n",
    "    \"\"\"\n",
    "    Extract tables from HTML as a single formatted text block for inclusion into page_text.\n",
    "    Skips navigation tables and handles no-table cases.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the file (for metadata).\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted block of all tables from this file, or a message if no tables are found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tables = pd.read_html(file_path)\n",
    "\n",
    "        def is_navigation_table(table):\n",
    "            \"\"\"Detect if table is a 'navigation-only' table with just 'back' and 'forward'.\"\"\"\n",
    "            flattened = [str(cell).strip().lower() for cell in table.to_numpy().flatten()]\n",
    "            navigation_keywords = {\"back\", \"forward\"}\n",
    "            return set(flattened).issubset(navigation_keywords)\n",
    "        \n",
    "        def is_nan_only_table(table):\n",
    "            \"\"\"Detect if the entire table only contains NaN values.\"\"\"\n",
    "            return table.isna().all().all()\n",
    "\n",
    "        table_texts = []\n",
    "        table_count = 0\n",
    "\n",
    "        for idx, table in enumerate(tables):\n",
    "            if is_navigation_table(table) or is_nan_only_table(table):\n",
    "                continue\n",
    "            \n",
    "            if table.shape[1] == 2:\n",
    "                # Drop rows where both the second and third columns are NaN\n",
    "                table = table.dropna(how='all')\n",
    "\n",
    "                last_col = table.columns[-1]\n",
    "\n",
    "                table[last_col] = table[last_col].fillna(\"\")\n",
    "\n",
    "            table_count += 1\n",
    "            formatted_table = tabulate(table, headers=\"keys\", tablefmt=\"grid\")\n",
    "\n",
    "            beautified_table = f\"\"\"\n",
    "╔════════════════════════════════════════════════════╗\n",
    "║            📊 Table {table_count} from {file_path}              ║\n",
    "╚════════════════════════════════════════════════════╝\n",
    "\n",
    "{formatted_table}\n",
    "\n",
    "╔════════════════════════════════════════════════════╗\n",
    "║            🔚 End of Table {table_count}                       ║\n",
    "╚════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "            table_texts.append(beautified_table)\n",
    "\n",
    "        if not table_texts:\n",
    "            return \"\"\n",
    "\n",
    "        return \"\\n\".join(table_texts)\n",
    "\n",
    "    except ValueError:\n",
    "        # No tables found case\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (5.3.1)\n",
      "Requirement already satisfied: html5lib in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from html5lib) (1.17.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_content():\n",
    "    \"\"\"\n",
    "    Load and process all .htm files from the given base directory for information retrieval.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of dictionaries containing processed text content and metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find all .htm files in the directory\n",
    "    htm_files = _list_htm_files()\n",
    "    logging.info(f\"Found {len(htm_files)} .htm files.\")\n",
    "\n",
    "    documents = []\n",
    "    page_texts = []\n",
    "\n",
    "    for file_path in htm_files:\n",
    "        try:\n",
    "            with open(file_path, encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "                # Extract content inside the <body> tag\n",
    "                content = content[content.find(\"<body>\") + 6 : content.find(\"</body>\")]\n",
    "\n",
    "                soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "                # Extract text, tables, and lists\n",
    "                clean_text = extract_text(soup)\n",
    "                formatted_table = extract_table_as_text_block(file_path)\n",
    "                lists = extract_list(soup)\n",
    "\n",
    "                # Combine extracted content\n",
    "                page_text = \"\\n\".join(filter(None, [clean_text, formatted_table, \"\\n\".join([\"• \" + item for sublist in lists for item in sublist])]))\n",
    "\n",
    "                page_texts.append(page_text)\n",
    "\n",
    "                # Store the processed content as a Document Objects. \n",
    "                document = Document(content=page_text, meta={\"source\": os.path.basename(file_path)})\n",
    "\n",
    "                documents.append(document)\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            logging.error(f\"Could not read the file {file_path}. Check the file encoding.\")\n",
    "\n",
    "    logging.info(f\"Processed {len(documents)} documents.\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HaochengLin\\AppData\\Local\\Temp\\ipykernel_26024\\174411885.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[last_col] = table[last_col].fillna(\"\")\n"
     ]
    }
   ],
   "source": [
    "training_documents = load_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Query and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers ollama-haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack_integrations.components.embedders.ollama import OllamaTextEmbedder\n",
    "from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
    "\n",
    "document_embedder = OllamaDocumentEmbedder()\n",
    "documents_with_embeddings = document_embedder.run(training_documents)['documents']\n",
    "document_store.write_documents(documents_with_embeddings, policy=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_pipeline = Pipeline()\n",
    "query_pipeline.add_component(\"text_embedder\", OllamaTextEmbedder())\n",
    "query_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "\n",
    "query = \"What is the limit to the number of curves in GEO?\"\n",
    "\n",
    "result = query_pipeline.run({\"text_embedder\":{\"text\": query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Ranking Document Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch,sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from haystack.components.rankers import TransformersSimilarityRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(content=\"Paris\"), Document(content=\"Berlin\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = TransformersSimilarityRanker()\n",
    "ranker.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.run(query=\"City in France\", documents=docs, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.run(query=\"I have 20000 modifiers added ty log, why I can't I add anymore?\", documents=training_documents, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Model (Offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-ollama) (0.3.47)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.3.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.10.6)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\haochenglin\\documents\\github\\geo-ir-model\\.venv\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_model = OllamaLLM(model=\"llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an AI assistant designed to help users navigate the GEO application.\n",
    "\n",
    "**Context:**  \n",
    "GEO is a well log authoring, analysis, and reporting system for petroleum geologists, geoscientists, and engineers.  \n",
    "Answer the user's question using **only** the provided documents.  \n",
    "\n",
    "**Instructions:**  \n",
    "- Use information from the **Documents** section to generate your response.  \n",
    "- Provide a **direct**, **concise**, and **factual** answer.  \n",
    "- **Avoid** speculative or unnecessary **explanations** or **justifications**.  \n",
    "- If the question is about a **numerical** or a **limit-based** constraint, return only the limit and its enforcement.  \n",
    "\n",
    "---\n",
    "**Documents:**  \n",
    "{documents}  \n",
    "---\n",
    "**Question:** {question}  \n",
    "---\n",
    "\n",
    "**Your Optimized Answer:**  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "with open(\"processed_content.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | offline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you are unable to add more than 20,000 modifiers to the log in GEO, it is likely due to a limitation on the maximum number of characters allowed for a single modifier. This limitation typically depends on the size of the data and the available storage space.\\n\\n**Possible Causes:**\\n\\n1. **Maximum Character Limit**: Each modifier may have a specific character limit, which could be exceeded when trying to add more modifiers.\\n2. **Data Storage Space**: The log file might not have enough free space to accommodate additional modifiers, causing the system to reject new additions.\\n3. **System Configuration**: Some system settings or configuration options might restrict the number of available slots for adding new modifiers.\\n\\n**Troubleshooting Steps:**\\n\\n1. Check the character limit for each modifier and ensure it does not exceed the allowed value.\\n2. Verify that there is sufficient free space in the log file to accommodate additional modifiers.\\n3. Adjust any system settings or configuration options related to data storage, character limits, or other restrictions.\\n\\n**Additional Information:** If you are still experiencing issues after checking these potential causes, consider consulting with your Geologix Limited representative for further assistance or guidance on optimizing your workflow.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"documents\": content,\n",
    "    \"question\": \"I have 20000 modifiers added ty log, why I can't I add anymore?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\n",
    "    \"documents\": content,\n",
    "    \"question\": \"What's the maximum curves I can load in a data file?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not provide information on the maximum number of curves that can be loaded in a data file.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There is no information provided about the limits to the number of tracks in the text. The only relevant information is related to saving and sharing ODF files, as well as the use of GEOView and GEOe-View. \\n\\nHowever, I can provide some general information that might be relevant:\\n\\n* In GEO, track layouts are saved in VEW files when they contain presentation data such as layout and formatting.\\n* There is no explicit limit mentioned on the number of tracks per file, but it's likely that there may be performance or technical limitations.\\n\\nIf you're looking for a more definitive answer, I would recommend checking the documentation or support resources provided by Geologix Limited, as they may have specific information on this topic.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"documents\": content,\n",
    "    \"question\": \"What are the limits to the number of tracks?\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
